<!DOCTYPE html>
<html>
<head>
    



    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Sandeep Chinchali</title>

    <meta name="description" content="Cloud Robotics">
    <!-- Bootstrap Core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous"> -->
    <!-- <link rel="stylesheet" href="/css/bootstrap.min.css"> -->
    <link href="https://fonts.googleapis.com/css?family=Montserrat:300,600" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,700" rel="stylesheet">

	<!-- JabRef CSS -->
	<link rel="stylesheet" href="https://stanfordasl.github.io/css/JabRef.css">
	 
    <link rel="canonical" href="http://asl.stanford.edu/people/sandeep-chinchali/"> 
    <!-- Not required -->
    <!-- <link rel="alternate" type="application/rss+xml" title="Autonomous Systems Laboratory" href="https://stanfordasl.github.io /feed.xml "> -->
    <!-- Custom Fonts -->
	<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <!-- <link href="css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    
    <!-- My CSS -->
    <link rel="stylesheet" href="https://stanfordasl.github.io/css/my.css"> 

    <!-- Analytics -->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124680200-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-124680200-1');
    </script>
    

    <!-- Favicons -->
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#2864b7">
    <meta name="msapplication-TileColor" content="#2864b7">
    <meta name="theme-color" content="#ffffff">

</head>





  <body id="page-top" class="index">
    <!-- Navigation -->
<nav id="mainNav" class="navbar navbar-default navbar-custom"> <!-- navbar-fixed-top -->
    <div class="container navbar-items">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="row">
            <div class="col-md-3" style="z-index: 1;">
                <div class="navbar-header page-scroll">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="/">
                        <img src="https://stanfordasl.github.io/img/ASL_Logo_White.png" alt="">
                    </a>
                </div>
            </div>
            <div class="col-md-9">
                <div class="row">
                    <!-- Collect the nav links, forms, and other content for toggling -->
                    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                        <ul class="nav navbar-nav navbar-right">
                            <li class="hidden">
                                <a href="#page-top"></a>
                            </li>
                            <!-- 
                            <li class="page-scroll">
                                <a href="/">Home</a>
                            </li>
                             -->
                            
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                                    
                                    <li class="page-scroll">
                                        <a href="/projects/">Projects</a>
                                    </li>
                                    
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                                    
                                    <li class="page-scroll">
                                        <a href="/publications/">Publications</a>
                                    </li>
                                    
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                                    
                                    <li class="page-scroll">
                                        <a href="/people/">People</a>
                                    </li>
                                    
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                            
                                
                                    
                                    <li class="page-scroll">
                                        <a href="/faq/">FAQ</a>
                                    </li>
                                    
                                
                            
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container-fluid -->
</nav>

    <header>
      <div class="container">
        <div class="row" style="margin-top: 40px;">
  	      <div class="col-md-5" style="margin-bottom: 20px;">
  	          <img src="../../img/people/SandeepChinchali.png" alt="Sandeep Chinchali" class="img-responsive">
            
            
  		    </div>
    		  <div class="col-md-7">
            <h2 style="margin-top: 0px;">Sandeep Chinchali</h2>
    		    <hr>
            <p>Sandeep is a postdoctoral scholar in the ASL Lab and will be an assistant professor in the ECE department at UT Austin starting in Fall 2021. Sandeep completed his PhD in computer science at Stanford, where he was advised by Marco Pavone and Sachin Katti. Previously, he was the first principal data scientist at Uhana, a Stanford startup working on data-driven optimization of cellular networks, now acquired by VMWare. Prior to Stanford, he graduated from Caltech, where he worked on robotics at NASA’s Jet Propulsion Lab (JPL). He is a recipient of the Stanford Graduate Fellowship and National Science Foundation (NSF) fellowships.</p>

            <br>
            
    		  </div>
  		</div>
      <div class="row">
        <div class="col-md-12">
        
          <h2 style="margin-top: 0px;">ASL Publications</h2>
          
          
          <ol class="bibliography"><li>
 


<span class="entry" id="NakanoyaChinchaliEtAl2021">M. Nakanoya, S. Chinchali, A. Anemogiannis, A. Datta, S. Katti, and M. Pavone, <a class="entry-title" href="https://arxiv.org/abs/2011.03216">“Task-relevant Representation Learning for Networked Robotic Perception,”</a> 2021. (Submitted)</span> 
<p class="infolinks">[<a href="javascript:toggleInfo('NakanoyaChinchaliEtAl2021','bibtex')">BibTeX</a>] [<a href="javascript:toggleInfo('NakanoyaChinchaliEtAl2021','abstract')">Abstract</a>]</p>
<span id="abs_NakanoyaChinchaliEtAl2021" class="abstract noshow">
	<p class="abstract"><b>Abstract:</b> Today, even the most compute-and-power constrained robots can measure complex, high data-rate video and LIDAR sensory streams. Often, such robots, ranging from low-power drones to space and subterranean rovers, need to transmit high-bitrate sensory data to a remote compute server if they are uncertain or cannot scalably run complex perception or mapping tasks locally. However, today’s representations for sensory data are mostly designed for human, not robotic, perception and thus often waste precious compute or wireless network resources to transmit unimportant parts of a scene that are unnecessary for a high-level robotic task. This paper presents an algorithm to learn task-relevant representations of sensory data that are co-designed with a pre-trained robotic perception model’s ultimate objective. Our algorithm aggressively compresses robotic sensory data by up to 11x more than competing methods. Further, it achieves high accuracy and robust generalization on diverse tasks including Mars terrain classification with low-power deep learning accelerators, neural motion planning, and environmental timeseries classification.</p>
</span>
<span id="bib_NakanoyaChinchaliEtAl2021" class="bibtex noshow">
	<pre>@inproceedings{NakanoyaChinchaliEtAl2021,
  author = {Nakanoya, M. and Chinchali, S. and Anemogiannis, A. and Datta, A. and Katti, S. and Pavone, M.},
  title = {Task-relevant Representation Learning for Networked Robotic Perception},
  year = {2021},
  note = {Submitted},
  url = {https://arxiv.org/abs/2011.03216},
  keywords = {sub},
  owner = {csandeep},
  timestamp = {2021-03-23}
}
</pre>
</span>
<!-- <span id="key_NakanoyaChinchaliEtAl2021" >
	<b>Keywords</b>:
	sub
</span> --></li>
<li>
 


<span class="entry" id="ChinchaliPergamentEtAl2020">S. Chinchali, E. Pergament, M. Nakanoya, E. Cidon, E. Zhang, D. Bharadia, M. Pavone, and S. Katti, <a class="entry-title" href="">“Sampling Training Data for Distributed Learning between Robots and the Cloud,”</a> in <i>Int. Symp. on Experimental Robotics</i>, Valetta, Malta, 2020. (In Press)</span> 
<p class="infolinks">[<a href="javascript:toggleInfo('ChinchaliPergamentEtAl2020','bibtex')">BibTeX</a>] [<a href="javascript:toggleInfo('ChinchaliPergamentEtAl2020','abstract')">Abstract</a>]</p>
<span id="abs_ChinchaliPergamentEtAl2020" class="abstract noshow">
	<p class="abstract"><b>Abstract:</b> Today’s robotic fleets are increasingly measuring high-volume video and LIDAR sensory streams, which can be mined for valuable training data, such as rare scenes of road construction sites, to steadily improve robotic perception models. However, re-training perception models on growing volumes of rich sensory data in central compute servers (or the "cloud") places an enormous time and cost burden on network transfer, cloud storage, human annotation, and cloud computing resources. Hence, we introduce HarvestNet, an intelligent sampling algorithm that resides on-board a robot and reduces system bottlenecks by only storing rare, useful events to steadily improve perception models re-trained in the cloud. HarvestNet significantly improves the accuracy of machine-learning models on our novel dataset of road construction sites, field testing of self-driving cars, and streaming face recognition, while reducing cloud storage, dataset annotation time, and cloud compute time by between 65.7-81.3%. Further, it is between 1.05-2.58x more accurate than baseline algorithms and scalably runs on embedded deep learning hardware.</p>
</span>
<span id="bib_ChinchaliPergamentEtAl2020" class="bibtex noshow">
	<pre>@inproceedings{ChinchaliPergamentEtAl2020,
  author = {Chinchali, S. and Pergament, E. and Nakanoya, M. and Cidon, E. and Zhang, E. and Bharadia, D. and Pavone, M. and Katti, S.},
  title = {Sampling Training Data for Distributed Learning between Robots and the Cloud},
  booktitle = {{Int. Symp. on Experimental Robotics}},
  year = {2020},
  note = {In press},
  address = {Valetta, Malta},
  month = mar,
  keywords = {press},
  owner = {csandeep},
  timestamp = {2020-11-09}
}
</pre>
</span>
<!-- <span id="key_ChinchaliPergamentEtAl2020" >
	<b>Keywords</b>:
	press
</span> --></li>
<li>
 


<span class="entry" id="ChinchaliSharmaEtAl2019">S. Chinchali, A. Sharma, J. Harrison, A. Elhafsi, D. Kang, E. Pergament, E. Cidon, S. Katti, and M. Pavone, <a class="entry-title" href="https://arxiv.org/pdf/1902.05703.pdf">“Network Offloading Policies for Cloud Robotics: a Learning-based Approach,”</a> in <i>Robotics: Science and Systems</i>, Freiburg im Breisgau, Germany, 2019.</span> 
<p class="infolinks">[<a href="javascript:toggleInfo('ChinchaliSharmaEtAl2019','bibtex')">BibTeX</a>] [<a href="javascript:toggleInfo('ChinchaliSharmaEtAl2019','abstract')">Abstract</a>]</p>
<span id="abs_ChinchaliSharmaEtAl2019" class="abstract noshow">
	<p class="abstract"><b>Abstract:</b> Today’s robotic systems are increasingly turning to computationally expensive models such as deep neural networks (DNNs) for tasks like localization, perception, planning, and object detection. However, resource-constrained robots, like low-power drones, often have insufficient on-board compute resources or power reserves to scalably run the most accurate, state-of-the art neural network compute models. Cloud robotics allows mobile robots the benefit of offloading compute to centralized servers if they are uncertain locally or want to run more accurate, compute-intensive models. However, cloud robotics comes with a key, often understated cost: communicating with the cloud over congested wireless networks may result in latency or loss of data. In fact, sending high data-rate video or LIDAR from multiple robots over congested networks can lead to prohibitive delay for real-time applications, which we measure experimentally. In this paper, we formulate a novel Robot Offloading Problem - how and when should robots offload sensing tasks, especially if they are uncertain, to improve accuracy while minimizing the cost of cloud communication? We formulate offloading as a sequential decision making problem for robots, and propose a solution using deep reinforcement learning. In both simulations and hardware experiments using state-of-the art vision DNNs, our offloading strategy improves vision task performance by between 1.3-2.6x of benchmark offloading strategies, allowing robots the potential to significantly transcend their on-board sensing accuracy but with limited cost of cloud communication.</p>
</span>
<span id="bib_ChinchaliSharmaEtAl2019" class="bibtex noshow">
	<pre>@inproceedings{ChinchaliSharmaEtAl2019,
  author = {Chinchali, S. and Sharma, A. and Harrison, J. and Elhafsi, A. and Kang, D. and Pergament, E. and Cidon, E. and Katti, S. and Pavone, M.},
  title = {Network Offloading Policies for Cloud Robotics: a Learning-based Approach},
  booktitle = {{Robotics: Science and Systems}},
  year = {2019},
  address = {Freiburg im Breisgau, Germany},
  month = jun,
  url = {https://arxiv.org/pdf/1902.05703.pdf},
  owner = {apoorva},
  timestamp = {2019-02-07}
}
</pre>
</span>
<!-- <span id="key_ChinchaliSharmaEtAl2019" >
	<b>Keywords</b>:
	
</span> --></li>
<li>
 


<span class="entry" id="ChinchaliLivingstonEtAl2018">S. P. Chinchali, S. C. Livingston, M. Chen, and M. Pavone, <a class="entry-title" href="/wp-content/papercite-data/pdf/Chinchali.Livingston.Chen.Pavone.IJRR18.pdf">“Multi-objective optimal control for proactive decision-making with temporal logic models,”</a> <i>Int. Journal of Robotics Research</i>, vol. 38, no. 12-13, pp. 1490–1512, 2019.</span> 
<p class="infolinks">[<a href="javascript:toggleInfo('ChinchaliLivingstonEtAl2018','bibtex')">BibTeX</a>] [<a href="javascript:toggleInfo('ChinchaliLivingstonEtAl2018','abstract')">Abstract</a>]</p>
<span id="abs_ChinchaliLivingstonEtAl2018" class="abstract noshow">
	<p class="abstract"><b>Abstract:</b> The operation of today’s robots entails interactions with humans, e.g., in autonomous driving amidst human-driven vehicles. To effectively do so, robots must proactively decode the intent of humans and concurrently leverage this knowledge for safe, cooperative task satisfaction—a problem we refer to as proactive decision making. However, simultaneous intent decoding and robotic control requires reasoning over several possible human behavioral models, resulting in high-dimensional state trajectories. In this paper, we address the proactive decision making problem using a novel combination of formal methods, control, and data mining techniques. First, we distill high-dimensional state trajectories of human-robot interaction into concise, symbolic behavioral summaries that can be learned from data. Second, we leverage formal methods to model high-level agent goals, safe interaction, and information-seeking behavior with temporal logic formulae. Finally, we design a novel decision-making scheme that maintains a belief distribution over models of human behavior, and proactively plans informative actions. After showing several desirable theoretical properties, we apply our framework to a dataset of humans driving in crowded merging scenarios. For it, temporal logic models are generated and used to synthesize control strategies using tree-based value iteration and deep reinforcement learning (RL). Additionally, we illustrate how data-driven models of human responses to informative robot probes, such as from generative models like Conditional Variational Autoencoders (CVAEs), can be clustered with formal specifications. Results from simulated self-driving car scenarios demonstrate that data-driven strategies enable safe interaction, correct model identification, and significant dimensionality reduction.</p>
</span>
<span id="bib_ChinchaliLivingstonEtAl2018" class="bibtex noshow">
	<pre>@article{ChinchaliLivingstonEtAl2018,
  author = {Chinchali, S. P. and Livingston, S. C. and Chen, M. and Pavone, M.},
  title = {Multi-objective optimal control for proactive decision-making with temporal logic models},
  journal = {{Int. Journal of Robotics Research}},
  volume = {38},
  number = {12-13},
  pages = {1490--1512},
  year = {2019},
  url = {/wp-content/papercite-data/pdf/Chinchali.Livingston.Chen.Pavone.IJRR18.pdf},
  owner = {SCL},
  timestamp = {2020-11-09}
}
</pre>
</span>
<!-- <span id="key_ChinchaliLivingstonEtAl2018" >
	<b>Keywords</b>:
	
</span> --></li>
<li>
 


<span class="entry" id="ChinchaliHuEtAl2018">S. Chinchali, P. Hu, T. Chu, M. Sharma, M. Bansal, R. Misra, M. Pavone, and Katti S, <a class="entry-title" href="/wp-content/papercite-data/pdf/Chinchali.ea.AAAI18.pdf">“Cellular Network Traffic Scheduling with Deep Reinforcement Learning,”</a> in <i>Proc. AAAI Conf. on Artificial Intelligence</i>, New Orleans, Louisiana, 2018.</span> 
<p class="infolinks">[<a href="javascript:toggleInfo('ChinchaliHuEtAl2018','bibtex')">BibTeX</a>] [<a href="javascript:toggleInfo('ChinchaliHuEtAl2018','abstract')">Abstract</a>]</p>
<span id="abs_ChinchaliHuEtAl2018" class="abstract noshow">
	<p class="abstract"><b>Abstract:</b> Modern mobile networks are facing unprecedented growth in demand due to a new class of traffic from Internet of Things (IoT) devices such as smart wearables and autonomous cars. Future networks must schedule delay-tolerant software updates, data backup, and other transfers from IoT devices while maintaining strict service guarantees for conventional real-time applications such as voice-calling and video. This problem is extremely challenging because conventional traffic is highly dynamic across space and time, so its performance is significantly impacted if all IoT traffic is scheduled immediately when it originates. In this paper, we present a reinforcement learning (RL) based scheduler that can dynamically adapt to traffic variation, and to various reward functions set by network operators, to optimally schedule IoT traffic. Using 4 weeks of real network data from downtown Melbourne, Australia spanning diverse traffic patterns, we demonstrate that our RL scheduler can enable mobile networks to carry 14.7% more data with minimal impact on existing traffic, and outperforms heuristic schedulers by more than 2x. Our work is a valuable step towards designing autonomous, "self- driving" networks that learn to manage themselves from past data.</p>
</span>
<span id="bib_ChinchaliHuEtAl2018" class="bibtex noshow">
	<pre>@inproceedings{ChinchaliHuEtAl2018,
  author = {Chinchali, S. and Hu, P. and Chu, T. and Sharma, M. and Bansal, M. and Misra, R. and Pavone, M. and Katti, S},
  title = {Cellular Network Traffic Scheduling with Deep Reinforcement Learning},
  booktitle = {{Proc. AAAI Conf. on Artificial Intelligence}},
  year = {2018},
  address = {New Orleans, Louisiana},
  month = feb,
  url = {/wp-content/papercite-data/pdf/Chinchali.ea.AAAI18.pdf},
  owner = {frossi2},
  timestamp = {2018-04-10}
}
</pre>
</span>
<!-- <span id="key_ChinchaliHuEtAl2018" >
	<b>Keywords</b>:
	
</span> --></li>
<li>
 


<span class="entry" id="ChinchaliLivingstonEtAl2017">S. P. Chinchali, S. C. Livingston, and M. Pavone, <a class="entry-title" href="/wp-content/papercite-data/pdf/Chinchali.Livingston.Pavone.ISRR17.pdf">“Multi-objective optimal control for proactive decision-making with temporal logic models,”</a> in <i>Int. Symp. on Robotics Research</i>, Puerto Varas, Chile, 2017.</span> 
<p class="infolinks">[<a href="javascript:toggleInfo('ChinchaliLivingstonEtAl2017','bibtex')">BibTeX</a>] [<a href="javascript:toggleInfo('ChinchaliLivingstonEtAl2017','abstract')">Abstract</a>]</p>
<span id="abs_ChinchaliLivingstonEtAl2017" class="abstract noshow">
	<p class="abstract"><b>Abstract:</b> The operation of today’s robots increasingly entails interactions with humans, in settings ranging from autonomous driving amidst human-driven vehicles to collaborative manufacturing. To effectively do so, robots must proactively decode the intent or plan of humans and concurrently leverage such a knowledge for safe, cooperative task satisfaction—a problem we refer to as proactive decision making. However, the problem of proactive intent decoding coupled with robotic control is computationally intractable as a robot must reason over several possible human behavioral models and resulting high-dimensional state trajectories. In this paper, we address the proactive decision making problem using a novel combination of algorithmic and data mining techniques. First, we distill high-dimensional state trajectories of human-robot interaction into concise, symbolic behavioral summaries that can be learned from data. Second, we leverage formal methods to model high-level agent goals, safe interaction, and information-seeking behavior with temporal logic formulae. Finally, we design a novel decision-making scheme that simply maintains a belief distribution over high-level, symbolic models of human behavior, and proactively plans informative control actions. Leveraging a rich dataset of real human driving data in crowded merging scenarios, we generate temporal logic models and use them to synthesize control strategies using tree-based value iteration and reinforcement learning (RL). Results from two simulated self-driving car scenarios, one cooperative and the other adversarial, demonstrate that our data-driven control strategies enable safe interaction, correct model identification, and significant dimensionality reduction.</p>
</span>
<span id="bib_ChinchaliLivingstonEtAl2017" class="bibtex noshow">
	<pre>@inproceedings{ChinchaliLivingstonEtAl2017,
  author = {Chinchali, S. P. and Livingston, S. C. and Pavone, M.},
  title = {Multi-objective optimal control for proactive decision-making with temporal logic models},
  booktitle = {{Int. Symp. on Robotics Research}},
  year = {2017},
  address = {Puerto Varas, Chile},
  month = dec,
  url = {/wp-content/papercite-data/pdf/Chinchali.Livingston.Pavone.ISRR17.pdf},
  owner = {pavone},
  timestamp = {2018-01-16}
}
</pre>
</span>
<!-- <span id="key_ChinchaliLivingstonEtAl2017" >
	<b>Keywords</b>:
	
</span> --></li>
<li>
 


<span class="entry" id="ChinchaliLivingstonEtAl2016">S. P. Chinchali, S. C. Livingston, M. Pavone, and J. W. Burdick, <a class="entry-title" href="/wp-content/papercite-data/pdf/Chinchali.Livingston.ea.ICRA16.pdf">“Simultaneous Model Identification and Task Satisfaction in the Presence of Temporal Logic Constraints,”</a> in <i>Proc. IEEE Conf. on Robotics and Automation</i>, Stockholm, Sweden, 2016.</span> 
<p class="infolinks">[<a href="javascript:toggleInfo('ChinchaliLivingstonEtAl2016','bibtex')">BibTeX</a>] [<a href="javascript:toggleInfo('ChinchaliLivingstonEtAl2016','abstract')">Abstract</a>]</p>
<span id="abs_ChinchaliLivingstonEtAl2016" class="abstract noshow">
	<p class="abstract"><b>Abstract:</b> Recent proliferation of cyber-physical systems, ranging from autonomous cars to nuclear hazard inspection robots, has exposed several challenging research problems on automated fault detection and recovery. This paper considers how recently developed formal synthesis and model verification techniques may be used to automatically generate information-seeking trajectories for anomaly detection. In particular, we consider the problem of how a robot could select its actions so as to maximally disambiguate between different model hypotheses that govern the environment it operates in or its interaction with other agents whose prime motivation is a priori unknown. The identification problem is posed as selection of the most likely model from a set of candidates, where each candidate is an adversarial Markov decision process (MDP) together with a linear temporal logic (LTL) formula that constrains robot-environment interaction. An adversarial MDP is an MDP in which transitions depend on both a (controlled) robot action and an (uncontrolled) adversary action. States are labeled, thus allowing interpretation of satisfaction of LTL formulae, which have a special form admitting satisfaction decisions in bounded time. An example where a robotic car must discern whether neighboring vehicles are following its trajectory for a surveillance operation is used to illustrate the problem and demonstrate our approach.</p>
</span>
<span id="bib_ChinchaliLivingstonEtAl2016" class="bibtex noshow">
	<pre>@inproceedings{ChinchaliLivingstonEtAl2016,
  author = {Chinchali, S. P. and Livingston, S. C. and Pavone, M. and Burdick, J. W.},
  title = {Simultaneous Model Identification and Task Satisfaction in the Presence of Temporal Logic Constraints},
  booktitle = {{Proc. IEEE Conf. on Robotics and Automation}},
  year = {2016},
  address = {Stockholm, Sweden},
  doi = {10.1109/ICRA.2016.7487553},
  month = may,
  url = {/wp-content/papercite-data/pdf/Chinchali.Livingston.ea.ICRA16.pdf},
  owner = {bylard},
  timestamp = {2017-01-28}
}
</pre>
</span>
<!-- <span id="key_ChinchaliLivingstonEtAl2016" >
	<b>Keywords</b>:
	
</span> --></li></ol>
        
        </div>
      </div>
	  </div>
    </header>
    <!--
The website should work without any of the JS below. -F
-->

<!-- jQuery -->
<script src="https://stanfordasl.github.io/js/jquery.min.js"></script>
<!-- Bootstrap Core JavaScript -->
<script src="https://stanfordasl.github.io/js/bootstrap.min.js"></script>
<!-- Plugin JavaScript -->
<!-- <script src="js/jquery.easing.min.js"></script> -->
<!-- Contact Form JavaScript -->
<!-- <script src="js/jqBootstrapValidation.js"></script> -->
<!-- <script src="js/contact_me.js"></script> -->
<!-- Custom Theme JavaScript -->
<!-- <script src="js/freelancer.min.js"></script> -->
<!-- JabRef (Toggling info on Publications Page) -->
<script src="https://stanfordasl.github.io/js/JabRef_QuickSearch.js"></script>

    <!-- Footer -->

<!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
<!-- <div class="scroll-top page-scroll hidden-lg hidden-md">
    <a class="btn btn-primary" href="#page-top">
        <i class="fa fa-chevron-up"></i>
    </a>
</div> -->

<!-- Made with (L) by Federico in October 2017
CSS: Bootstrap (without the JS)
http://getbootstrap.com/
Bloated, but so convenient.
Plus, it's probably already in your browser cache (hence the CDN).
If you want the Jekyll project for your own website,
feel free to drop me a line!
-F
-->

<!-- <hr class="footer-line"/> -->
<script type="text/javascript">
function footerAlign() {
  $('footer').css('display', 'block');
  $('footer').css('height', 'auto');
  var footerHeight = $('footer').outerHeight();
  $('body').css('padding-bottom', footerHeight);
  $('footer').css('height', footerHeight);
}


$(document).ready(function(){
  footerAlign();
});

$( window ).resize(function() {
  footerAlign();
});
</script>

<footer class="footer">
<div id="social-icons">
  <a href="https://twitter.com/stanfordasl" target="_blank"><img src="https://stanfordasl.github.io/img/ASL_twitter.png" alt=""></a>
  <a href="https://www.instagram.com/stanfordasl" target="_blank"><img src="https://stanfordasl.github.io/img/ASL_instagram.png" alt=""></a>
  <a href="https://github.com/StanfordASL" target="_blank"><img src="https://stanfordasl.github.io/img/ASL_github.png" alt=""></a>
</div>
<div class="copyright-text">
  © Autonomous Systems Lab 2021. All rights reserved.
</div>
</footer>
  </body>
</html>
